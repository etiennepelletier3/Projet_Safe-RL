{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./ppo_tb/PPO_6\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.0361  |\n",
      "| time/              |          |\n",
      "|    fps             | 733      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaos/miniconda3/envs/safe_rl/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=-26.77 +/- 2.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -26.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004337332 |\n",
      "|    clip_fraction        | 0.021       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -9.75       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00613     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.14    |\n",
      "| time/              |          |\n",
      "|    fps             | 585      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-27.78 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -27.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043371115 |\n",
      "|    clip_fraction        | 0.00776      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -0.574       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00466      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 0.0195       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.189   |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-9.82 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -9.82        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028500862 |\n",
      "|    clip_fraction        | 0.00244      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -0.882       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00402      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000855    |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.014        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.249   |\n",
      "| time/              |          |\n",
      "|    fps             | 542      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-2.42 +/- 1.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -2.42        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041593304 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -0.359       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00304      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 0.00947      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.0949  |\n",
      "| time/              |          |\n",
      "|    fps             | 534      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -0.0887      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045744795 |\n",
      "|    clip_fraction        | 0.0102       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -0.325       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.003        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 0.0132       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-0.33 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -0.334       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037205967 |\n",
      "|    clip_fraction        | 0.0161       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -0.954       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00108     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 0.00829      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-21.16 +/- 3.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -21.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040732194 |\n",
      "|    clip_fraction        | 0.0147       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | -0.664       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000625     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 0.00753      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.0643  |\n",
      "| time/              |          |\n",
      "|    fps             | 547      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-26.91 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -26.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011907689 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | -0.738       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00268      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000681    |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 0.00853      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.0325  |\n",
      "| time/              |          |\n",
      "|    fps             | 542      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 135      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-28.60 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -28.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025316407 |\n",
      "|    clip_fraction        | 0.0024       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -0.326       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00287      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 0.00846      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -0.0331  |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-26.85 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -26.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004518569 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | -0.585      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0012     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 0.00698     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 0.0682   |\n",
      "| time/              |          |\n",
      "|    fps             | 533      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 0.0898       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 549          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 178          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028892646 |\n",
      "|    clip_fraction        | 0.00459      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -0.395       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00145      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 0.00873      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-28.04 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -28          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034907856 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -0.92        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.11e-05    |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 0.00514      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 0.0888   |\n",
      "| time/              |          |\n",
      "|    fps             | 545      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-28.09 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -28.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 110000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00402264 |\n",
      "|    clip_fraction        | 0.0452     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.79      |\n",
      "|    explained_variance   | -1.57      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00401   |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.00405   |\n",
      "|    std                  | 0.978      |\n",
      "|    value_loss           | 0.00557    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 0.147    |\n",
      "| time/              |          |\n",
      "|    fps             | 542      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-27.48 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -27.5        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025384077 |\n",
      "|    clip_fraction        | 0.0176       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | -0.526       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000763     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 0.00819      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 0.317    |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-27.29 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -27.3        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 130000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024680411 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | -0.592       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000588    |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 0.00742      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 0.354    |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 244      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 0.409        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 254          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032887843 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | -0.764       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00161     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 0.00514      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-27.89 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -27.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031968765 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | -0.214       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00163      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 0.0106       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 0.541    |\n",
      "| time/              |          |\n",
      "|    fps             | 544      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-20.70 +/- 10.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -20.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073678796 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | -0.115       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000264    |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00265     |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 0.00932      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 0.665    |\n",
      "| time/              |          |\n",
      "|    fps             | 542      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-10.69 +/- 13.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -10.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055265566 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | -0.0454      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00393     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00331     |\n",
      "|    std                  | 0.968        |\n",
      "|    value_loss           | 0.00985      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 0.942    |\n",
      "| time/              |          |\n",
      "|    fps             | 540      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-3.17 +/- 3.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -3.17        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 170000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046193236 |\n",
      "|    clip_fraction        | 0.0616       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.0767       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00182      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 0.0167       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 320      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-6.74 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -6.74        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 180000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017263534 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.0845       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00175      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 0.0141       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 1.92         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 346          |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028781565 |\n",
      "|    clip_fraction        | 0.0224       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.0936       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00673      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 0.0209       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-10.23 +/- 9.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -10.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005481588 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.007       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    std                  | 0.962       |\n",
      "|    value_loss           | 0.0239      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 541      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-6.98 +/- 10.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -6.98       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005087306 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00752     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00265    |\n",
      "|    std                  | 0.962       |\n",
      "|    value_loss           | 0.0264      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 540      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 379      |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-6.72 +/- 13.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -6.72        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 210000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043519777 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.367        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00806      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00268     |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 0.0284       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 395      |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-1.09 +/- 8.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -1.09       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006159896 |\n",
      "|    clip_fraction        | 0.0328      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0111      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00318    |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.0332      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.96     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 4.51         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 422          |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038593856 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.313        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00776      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    std                  | 0.952        |\n",
      "|    value_loss           | 0.0296       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=7.18 +/- 3.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 7.18         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 230000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032406556 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.418        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0095       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    std                  | 0.946        |\n",
      "|    value_loss           | 0.0297       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 541      |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 438      |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=11.54 +/- 4.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 11.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005461791 |\n",
      "|    clip_fraction        | 0.0401      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00856     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 0.0369      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 540      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 454      |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=12.11 +/- 3.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 12.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 250000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041748947 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.555        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0106       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    std                  | 0.937        |\n",
      "|    value_loss           | 0.0338       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.75     |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 471      |\n",
      "|    total_timesteps | 253952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=11.24 +/- 7.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 11.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 260000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046538003 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0098       |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00329     |\n",
      "|    std                  | 0.936        |\n",
      "|    value_loss           | 0.0385       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 488      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=19.15 +/- 2.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 19.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005658893 |\n",
      "|    clip_fraction        | 0.0218      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0037     |\n",
      "|    std                  | 0.933       |\n",
      "|    value_loss           | 0.0413      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 504      |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 8.84         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 514          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046489895 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.521        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0143       |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    std                  | 0.933        |\n",
      "|    value_loss           | 0.0408       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=19.13 +/- 1.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 19.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 280000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052951425 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0172       |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    std                  | 0.929        |\n",
      "|    value_loss           | 0.0481       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 540      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 530      |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=18.16 +/- 2.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 18.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 290000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052020326 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.371        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0128       |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    std                  | 0.924        |\n",
      "|    value_loss           | 0.0435       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 10.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 547      |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=17.25 +/- 5.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 17.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 300000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027947444 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.485        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0139       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00264     |\n",
      "|    std                  | 0.921        |\n",
      "|    value_loss           | 0.0426       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 10.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 563      |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=19.83 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 19.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 310000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00523631 |\n",
      "|    clip_fraction        | 0.0336     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.67      |\n",
      "|    explained_variance   | 0.48       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0153     |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.00356   |\n",
      "|    std                  | 0.916      |\n",
      "|    value_loss           | 0.0459     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 11.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 580      |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 12.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 590          |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034481147 |\n",
      "|    clip_fraction        | 0.0134       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.506        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0151       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    std                  | 0.912        |\n",
      "|    value_loss           | 0.0455       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=18.43 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 18.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034331859 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.487        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.014        |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00306     |\n",
      "|    std                  | 0.91         |\n",
      "|    value_loss           | 0.0436       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 12.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 540      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 606      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=19.93 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 19.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 330000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032885666 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.56         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0122       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    std                  | 0.906        |\n",
      "|    value_loss           | 0.0378       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 13.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 622      |\n",
      "|    total_timesteps | 335872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=18.42 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 18.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003939902 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.014       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    std                  | 0.902       |\n",
      "|    value_loss           | 0.0399      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 14.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 639      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=19.21 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 19.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 350000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026288605 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.593        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00807      |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    std                  | 0.898        |\n",
      "|    value_loss           | 0.0363       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 14.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 656      |\n",
      "|    total_timesteps | 352256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=19.91 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 19.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037796989 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.597        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00801      |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.0048      |\n",
      "|    std                  | 0.895        |\n",
      "|    value_loss           | 0.0354       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 15       |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 672      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 15.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 682          |\n",
      "|    total_timesteps      | 368640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031365848 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.67         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00782      |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    std                  | 0.891        |\n",
      "|    value_loss           | 0.0319       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=20.32 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 20.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 370000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005344484 |\n",
      "|    clip_fraction        | 0.0278      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0106      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    std                  | 0.888       |\n",
      "|    value_loss           | 0.0343      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 15.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 698      |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=20.30 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 20.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036111851 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.59        |\n",
      "|    explained_variance   | 0.634        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0121       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00386     |\n",
      "|    std                  | 0.882        |\n",
      "|    value_loss           | 0.0413       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 16.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 715      |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=21.15 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 21.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 390000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041250014 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.52         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00405     |\n",
      "|    std                  | 0.881        |\n",
      "|    value_loss           | 0.0408       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 16.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 732      |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=21.75 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 21.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038856748 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.658        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00696      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    std                  | 0.877        |\n",
      "|    value_loss           | 0.0343       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 16.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 748      |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 17.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 539          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 758          |\n",
      "|    total_timesteps      | 409600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049563395 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.698        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.008        |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    std                  | 0.872        |\n",
      "|    value_loss           | 0.0317       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=21.33 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 21.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 410000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035207402 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.56        |\n",
      "|    explained_variance   | 0.733        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00635      |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00433     |\n",
      "|    std                  | 0.869        |\n",
      "|    value_loss           | 0.0306       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 17.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 774      |\n",
      "|    total_timesteps | 417792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=19.99 +/- 1.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 20           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045366837 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.742        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0053       |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    std                  | 0.864        |\n",
      "|    value_loss           | 0.0308       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 17.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 791      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=21.79 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 21.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 430000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055347085 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.716        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00708      |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00446     |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 0.0323       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 808      |\n",
      "|    total_timesteps | 434176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=20.83 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 20.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005316832 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00431     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0043     |\n",
      "|    std                  | 0.856       |\n",
      "|    value_loss           | 0.0252      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 18       |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 824      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=22.63 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 22.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 450000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039211623 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00472      |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    std                  | 0.853        |\n",
      "|    value_loss           | 0.0244       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 840      |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 18.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 539         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 850         |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004065814 |\n",
      "|    clip_fraction        | 0.0337      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00431     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    std                  | 0.85        |\n",
      "|    value_loss           | 0.0258      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=21.34 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 21.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 460000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054824743 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.764        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00907      |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    std                  | 0.844        |\n",
      "|    value_loss           | 0.0319       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 19.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 867      |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=22.13 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 22.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 470000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043423744 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | 0.787        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00716      |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00314     |\n",
      "|    std                  | 0.84         |\n",
      "|    value_loss           | 0.0301       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 883      |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=24.40 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 24.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005425877 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00607     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    std                  | 0.838       |\n",
      "|    value_loss           | 0.0291      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 900      |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=21.88 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 21.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 490000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053198976 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.77         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00523      |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.0047      |\n",
      "|    std                  | 0.837        |\n",
      "|    value_loss           | 0.0295       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 916      |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 20           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 539          |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 926          |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059726164 |\n",
      "|    clip_fraction        | 0.0361       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00765      |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00528     |\n",
      "|    std                  | 0.833        |\n",
      "|    value_loss           | 0.029        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=23.57 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 23.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 500000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054242536 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00247      |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00492     |\n",
      "|    std                  | 0.832        |\n",
      "|    value_loss           | 0.0252       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 20.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 942      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=24.16 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 24.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 510000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004573595 |\n",
      "|    clip_fraction        | 0.026       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00333     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00471    |\n",
      "|    std                  | 0.828       |\n",
      "|    value_loss           | 0.0261      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 959      |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=24.14 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 24.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 520000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061073857 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.823        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00828      |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00393     |\n",
      "|    std                  | 0.826        |\n",
      "|    value_loss           | 0.0268       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 976      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=25.68 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 530000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054951767 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00625      |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    std                  | 0.823        |\n",
      "|    value_loss           | 0.0296       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 21.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 992      |\n",
      "|    total_timesteps | 532480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=23.15 +/- 1.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 23.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004785003 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.44       |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00166     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    std                  | 0.82        |\n",
      "|    value_loss           | 0.0221      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 21.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1008     |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 21.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 538         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 1019        |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003794818 |\n",
      "|    clip_fraction        | 0.017       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.44       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00475     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    std                  | 0.817       |\n",
      "|    value_loss           | 0.0223      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=24.76 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 24.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 550000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046437364 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.43        |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00231      |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.0044      |\n",
      "|    std                  | 0.814        |\n",
      "|    value_loss           | 0.0247       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1035     |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=23.47 +/- 1.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 23.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005329944 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00261     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00427    |\n",
      "|    std                  | 0.812       |\n",
      "|    value_loss           | 0.023       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 1052     |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=25.06 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 570000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045173634 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.42        |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00693      |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    std                  | 0.811        |\n",
      "|    value_loss           | 0.0245       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1068     |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=25.23 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 25.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004087383 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0019      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00502    |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 0.0218      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 1084     |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 23           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 1095         |\n",
      "|    total_timesteps      | 589824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048876884 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.41        |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00492      |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00377     |\n",
      "|    std                  | 0.805        |\n",
      "|    value_loss           | 0.0214       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=25.16 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 590000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048008235 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.4         |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00548      |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00363     |\n",
      "|    std                  | 0.802        |\n",
      "|    value_loss           | 0.0261       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 23.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 1111     |\n",
      "|    total_timesteps | 598016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=25.13 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 600000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039106123 |\n",
      "|    clip_fraction        | 0.0541       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.39        |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000916    |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    std                  | 0.798        |\n",
      "|    value_loss           | 0.0183       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1128     |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=25.18 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 25.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 610000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004775597 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000309    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 0.0228      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 1144     |\n",
      "|    total_timesteps | 614400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=25.68 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 620000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035623768 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.38        |\n",
      "|    explained_variance   | 0.793        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00738      |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00383     |\n",
      "|    std                  | 0.795        |\n",
      "|    value_loss           | 0.0273       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 1160     |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=23.77 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 23.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 630000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0036021 |\n",
      "|    clip_fraction        | 0.0305    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.38     |\n",
      "|    explained_variance   | 0.817     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00281   |\n",
      "|    n_updates            | 760       |\n",
      "|    policy_gradient_loss | -0.00442  |\n",
      "|    std                  | 0.793     |\n",
      "|    value_loss           | 0.0231    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 1177     |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 24.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 1187         |\n",
      "|    total_timesteps      | 638976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042354255 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.37        |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00351      |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    std                  | 0.79         |\n",
      "|    value_loss           | 0.0216       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=26.76 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038342346 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.36        |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00158      |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00329     |\n",
      "|    std                  | 0.789        |\n",
      "|    value_loss           | 0.0202       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 1204     |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=25.93 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 650000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035985834 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.36        |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00172      |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.0037      |\n",
      "|    std                  | 0.786        |\n",
      "|    value_loss           | 0.019        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1220     |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=26.49 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005978329 |\n",
      "|    clip_fraction        | 0.0451      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00216     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    std                  | 0.784       |\n",
      "|    value_loss           | 0.0195      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 1237     |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=25.98 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004576396 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00163     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00316    |\n",
      "|    std                  | 0.783       |\n",
      "|    value_loss           | 0.0178      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1253     |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 24.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 1263         |\n",
      "|    total_timesteps      | 679936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031756815 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.35        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00374      |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    std                  | 0.782        |\n",
      "|    value_loss           | 0.0203       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=26.15 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 680000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055164206 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.34        |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00618      |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    std                  | 0.779        |\n",
      "|    value_loss           | 0.024        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1280     |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=25.28 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 690000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034340234 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00203      |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    std                  | 0.775        |\n",
      "|    value_loss           | 0.0169       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 1296     |\n",
      "|    total_timesteps | 696320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=24.84 +/- 1.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 24.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 700000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042033223 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000744     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    std                  | 0.773        |\n",
      "|    value_loss           | 0.0172       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1312     |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=25.42 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 25.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 710000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005200683 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00158     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00403    |\n",
      "|    std                  | 0.772       |\n",
      "|    value_loss           | 0.0181      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 1329     |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=26.38 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 720000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051384615 |\n",
      "|    clip_fraction        | 0.0345       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.32        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00131      |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.00292     |\n",
      "|    std                  | 0.772        |\n",
      "|    value_loss           | 0.0148       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1345     |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 25.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1356        |\n",
      "|    total_timesteps      | 729088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005448199 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00329     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.00465    |\n",
      "|    std                  | 0.769       |\n",
      "|    value_loss           | 0.0207      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=25.72 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 730000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059956843 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.31        |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00132      |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    std                  | 0.766        |\n",
      "|    value_loss           | 0.0168       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1372     |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=25.74 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 740000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042310935 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.3         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000588     |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00326     |\n",
      "|    std                  | 0.764        |\n",
      "|    value_loss           | 0.0168       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 1389     |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=26.16 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 750000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004421553 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.3        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.21e-05   |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    std                  | 0.764       |\n",
      "|    value_loss           | 0.0165      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1405     |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=24.91 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 24.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 760000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043159556 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.3         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00223      |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    std                  | 0.762        |\n",
      "|    value_loss           | 0.019        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 1421     |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=24.36 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 24.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 770000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050797034 |\n",
      "|    clip_fraction        | 0.0307       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.29        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00172     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.00377     |\n",
      "|    std                  | 0.76         |\n",
      "|    value_loss           | 0.0137       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 1438     |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 25.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 537          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 1449         |\n",
      "|    total_timesteps      | 778240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053263395 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.28        |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000589     |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00382     |\n",
      "|    std                  | 0.757        |\n",
      "|    value_loss           | 0.0176       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=27.32 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 27.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 780000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048988927 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.28        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00191      |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.00384     |\n",
      "|    std                  | 0.755        |\n",
      "|    value_loss           | 0.0145       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1465     |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=26.13 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 790000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003861004 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.27       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000796   |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    std                  | 0.753       |\n",
      "|    value_loss           | 0.013       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 1481     |\n",
      "|    total_timesteps | 794624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=26.00 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 800000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055137007 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.27        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00082      |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00442     |\n",
      "|    std                  | 0.749        |\n",
      "|    value_loss           | 0.0166       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1498     |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=26.64 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 810000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005093402 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000542    |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    std                  | 0.745       |\n",
      "|    value_loss           | 0.0159      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 1515     |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 25.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1525        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003966308 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.24       |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00105    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.00424    |\n",
      "|    std                  | 0.742       |\n",
      "|    value_loss           | 0.018       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=25.36 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 25.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005946453 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.24       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00212     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.00316    |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 0.0157      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 1541     |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=26.00 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 830000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003920978 |\n",
      "|    clip_fraction        | 0.0263      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.24       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000113    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    std                  | 0.74        |\n",
      "|    value_loss           | 0.0152      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1558     |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=26.37 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004341674 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00261    |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.00403    |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 0.0138      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 1574     |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=26.15 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 850000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048107365 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.23        |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00038      |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.00446     |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 0.0172       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 1590     |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=25.90 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 25.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004548189 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000903    |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    std                  | 0.736       |\n",
      "|    value_loss           | 0.0161      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 1607     |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 25.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 536         |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 1617        |\n",
      "|    total_timesteps      | 868352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004008079 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.22       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000905    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00339    |\n",
      "|    std                  | 0.735       |\n",
      "|    value_loss           | 0.0139      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=25.67 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 25.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 870000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005776823 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.22       |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00139    |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    std                  | 0.733       |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 1634     |\n",
      "|    total_timesteps | 876544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=25.95 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 880000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053797495 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.21        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00233     |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | -0.00391     |\n",
      "|    std                  | 0.731        |\n",
      "|    value_loss           | 0.014        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1650     |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=26.21 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 890000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052971486 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.21        |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00116     |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    std                  | 0.73         |\n",
      "|    value_loss           | 0.0148       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 1666     |\n",
      "|    total_timesteps | 892928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=27.16 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 27.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003663806 |\n",
      "|    clip_fraction        | 0.0249      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.2        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00123     |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    std                  | 0.727       |\n",
      "|    value_loss           | 0.0178      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1683     |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 25.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 536         |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 1694        |\n",
      "|    total_timesteps      | 909312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004449929 |\n",
      "|    clip_fraction        | 0.0319      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.2        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000193    |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 0.0156      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=26.66 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 910000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048824227 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.19        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00219     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.00443     |\n",
      "|    std                  | 0.723        |\n",
      "|    value_loss           | 0.0146       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 1710     |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=25.92 +/- 1.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 25.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005107634 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00142    |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    std                  | 0.722       |\n",
      "|    value_loss           | 0.0123      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 1726     |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=26.30 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 930000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053683138 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.18        |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00415     |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.00358     |\n",
      "|    std                  | 0.72         |\n",
      "|    value_loss           | 0.0117       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1743     |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=25.57 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 25.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 940000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043261927 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.18        |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000395    |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    std                  | 0.718        |\n",
      "|    value_loss           | 0.0134       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 26       |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 1760     |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=25.79 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 25.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 950000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004618396 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00331    |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.00406    |\n",
      "|    std                  | 0.716       |\n",
      "|    value_loss           | 0.00993     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 534      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 1776     |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 26.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 536         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 1786        |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005501439 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00138     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    std                  | 0.713       |\n",
      "|    value_loss           | 0.0165      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=26.72 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 960000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064331563 |\n",
      "|    clip_fraction        | 0.0432       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.16        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.23e-05     |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -0.00391     |\n",
      "|    std                  | 0.713        |\n",
      "|    value_loss           | 0.0132       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1803     |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=26.36 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 970000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005278039 |\n",
      "|    clip_fraction        | 0.0485      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00372    |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.00432    |\n",
      "|    std                  | 0.713       |\n",
      "|    value_loss           | 0.0118      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 1819     |\n",
      "|    total_timesteps | 974848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=26.44 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 980000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055858116 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.16        |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00124      |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.00358     |\n",
      "|    std                  | 0.711        |\n",
      "|    value_loss           | 0.0147       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1836     |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=26.78 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 26.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 990000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004696855 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.15       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000789   |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.00412    |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 1852     |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 26.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 122        |\n",
      "|    time_elapsed         | 1862       |\n",
      "|    total_timesteps      | 999424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00628654 |\n",
      "|    clip_fraction        | 0.0577     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.15      |\n",
      "|    explained_variance   | 0.892      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00362   |\n",
      "|    n_updates            | 1210       |\n",
      "|    policy_gradient_loss | -0.00411   |\n",
      "|    std                  | 0.707      |\n",
      "|    value_loss           | 0.0112     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=26.18 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 26.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1000000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051847813 |\n",
      "|    clip_fraction        | 0.0419       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.14        |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000553     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.00418     |\n",
      "|    std                  | 0.704        |\n",
      "|    value_loss           | 0.016        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 1879     |\n",
      "|    total_timesteps | 1007616  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import safety_gymnasium\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "ENV_ID = \"SafetyPointGoal1-v0\"     # tâche simple, contrainte de collision\n",
    "\n",
    "import gymnasium as gym\n",
    "import safety_gymnasium\n",
    "from safety_gymnasium.wrappers import SafetyGymnasium2Gymnasium\n",
    "\n",
    "def make_env():\n",
    "    safe_env = safety_gymnasium.make(\"SafetyPointGoal1-v0\", render_mode=None)\n",
    "    env = SafetyGymnasium2Gymnasium(safe_env)   # ⇨ API Gymnasium\n",
    "    return env\n",
    "if __name__ == \"__main__\":\n",
    "    train_env = make_env()\n",
    "    eval_env  = make_env()\n",
    "\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=train_env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./ppo/ppo_tb\",\n",
    "        batch_size=4096,\n",
    "        n_steps=8192,\n",
    "    )\n",
    "\n",
    "    eval_cb = EvalCallback(\n",
    "        eval_env,\n",
    "        eval_freq=10_000,\n",
    "        best_model_save_path=\"./ppo/ppo_best\",\n",
    "        log_path=\"./ppo/ppo_eval\",\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=1_000_000, callback=eval_cb)\n",
    "    model.save(\"ppo_safety_point_goal1\")\n",
    "\n",
    "    train_env.close(); eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaos/miniconda3/envs/safe_rl/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/chaos/Desktop/Projet_Safe-RL folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  saved → /home/chaos/Desktop/Projet_Safe-RL/ppo_rollout_3rd-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"           # or osmesa\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "import safety_gymnasium\n",
    "from safety_gymnasium.wrappers import SafetyGymnasium2Gymnasium\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# --- 1. 3rd‑person env ----------------------------------------------------\n",
    "base_env = safety_gymnasium.make(\n",
    "    \"SafetyPointGoal1-v0\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    camera_name=\"track\",          # <‑‑ HERE: 3rd‑person chase camera\n",
    ")\n",
    "env = SafetyGymnasium2Gymnasium(base_env)\n",
    "\n",
    "# --- 2. Record exactly one episode ---------------------------------------\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=\"/home/chaos/Desktop/Projet_Safe-RL\",\n",
    "    episode_trigger=lambda ep: ep == 0,\n",
    "    name_prefix=\"ppo_rollout_3rd\",\n",
    "    disable_logger=True,\n",
    ")\n",
    "\n",
    "# --- 3. Load policy & roll ~30 s (900 steps) ------------------------------\n",
    "model = PPO.load(\"ppo/ppo_best/best_model\")   # best checkpoint\n",
    "\n",
    "obs, _ = env.reset(seed=0)\n",
    "for _ in range(900):                      # 30 s @ 30 FPS\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, _, done, truncated, _ = env.step(action)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(\"✅  saved → /home/chaos/Desktop/Projet_Safe-RL/ppo_rollout_3rd-episode-0.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load(\"ppo_eval/evaluations.npz\")\n",
    "timesteps      = data[\"timesteps\"]          # shape (k,)\n",
    "results        = data[\"results\"]            # shape (k, n_eval_episodes)\n",
    "mean_rewards   = results.mean(axis=1)\n",
    "std_rewards    = results.std(axis=1)\n",
    "\n",
    "plt.plot(timesteps, mean_rewards, label=\"mean reward\")\n",
    "plt.fill_between(\n",
    "    timesteps,\n",
    "    mean_rewards - std_rewards,\n",
    "    mean_rewards + std_rewards,\n",
    "    alpha=0.3,\n",
    "    label=\"±1 σ\",\n",
    ")\n",
    "plt.xlabel(\"environment steps\")\n",
    "plt.ylabel(\"episode reward\")\n",
    "plt.title(\"PPO – SafetyPointGoal1\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## omnisafe train --algo CPO --env-id SafetyPointGoal1-v0 --total-steps 1000000 --vector-env-nums 1 --device cuda:0 --log-dir cpo_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/chaos/Desktop/Projet_Safe-RL\n",
      "Contents of './cpo_logs/train/CPO-{SafetyPointGoal1-v0}/seed-000-2025-04-13-22-53-59/torch_save': ['epoch-0.pt', 'epoch-50.pt']\n",
      "Observation dimension: 60, Action dimension: 2\n",
      "Policy state dict keys: ['log_std', 'mean.0.weight', 'mean.0.bias', 'mean.2.weight', 'mean.2.bias', 'mean.4.weight', 'mean.4.bias']\n",
      "Missing keys after loading: ['net.0.weight', 'net.0.bias', 'net.2.weight', 'net.2.bias', 'net.4.weight', 'net.4.bias']\n",
      "Unexpected keys after loading: ['log_std', 'mean.0.weight', 'mean.0.bias', 'mean.2.weight', 'mean.2.bias', 'mean.4.weight', 'mean.4.bias']\n",
      "✅ Successfully loaded the CPO model.\n",
      "✅ Video saved → ./cpo_logs/video/cpo_rollout_3rd-episode-0.mp4\n",
      "Evaluation log file not found at ./cpo_logs/evaluations.npz. Skipping evaluation plot.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import safety_gymnasium\n",
    "from safety_gymnasium.wrappers import SafetyGymnasium2Gymnasium\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Environment variables for headless rendering (if applicable)\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"           # or \"osmesa\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Define the policy network architecture that matches OmniSafe's checkpoint.\n",
    "class CPOPolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_sizes=(64, 64)):\n",
    "        super(CPOPolicyNetwork, self).__init__()\n",
    "        layers = []\n",
    "        last_dim = obs_dim\n",
    "        # Build the network and assign it to self.mean so that the keys match.\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_dim = hidden_size\n",
    "        layers.append(nn.Linear(last_dim, action_dim))\n",
    "        self.mean = nn.Sequential(*layers)\n",
    "        # Create a learnable log_std parameter (one per action dimension)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Return the mean of the policy.\n",
    "        return self.mean(x)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Wrapper to provide a predict() method similar to Stable Baselines3.\n",
    "class CPOModuleWrapper:\n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        # Convert observation (NumPy array) to a torch tensor with a batch dimension.\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            # Use the model's forward method to get the mean action.\n",
    "            action_tensor = self.model(obs_tensor)\n",
    "        # Remove the batch dimension and convert the result to NumPy.\n",
    "        action = action_tensor.squeeze(0).cpu().numpy()\n",
    "        return action, None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to load the OmniSafe CPO model checkpoint.\n",
    "def load_cpo_model(checkpoint_path: str, obs_dim: int, action_dim: int) -> CPOModuleWrapper:\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint file was not found at: {checkpoint_path}. \"\n",
    "            \"Please verify the file path and name, or check if the model was saved correctly.\"\n",
    "        )\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    # If the checkpoint is a dictionary with key \"pi\", extract the policy state dict.\n",
    "    if isinstance(checkpoint, dict) and \"pi\" in checkpoint:\n",
    "        policy_state_dict = checkpoint[\"pi\"]\n",
    "    else:\n",
    "        policy_state_dict = checkpoint\n",
    "\n",
    "    # Print out the keys of the loaded state dictionary for inspection.\n",
    "    print(\"Policy state dict keys:\", list(policy_state_dict.keys()))\n",
    "    \n",
    "    # Instantiate the model with the new architecture.\n",
    "    model = CPOPolicyNetwork(obs_dim, action_dim, hidden_sizes=(64, 64))\n",
    "    \n",
    "    # Load the state dictionary with strict=True now that the keys should match.\n",
    "    load_info = model.load_state_dict(policy_state_dict, strict=False)\n",
    "    print(\"Missing keys after loading:\", load_info.missing_keys)\n",
    "    print(\"Unexpected keys after loading:\", load_info.unexpected_keys)\n",
    "    \n",
    "    return CPOModuleWrapper(model)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to set up the environment with video recording.\n",
    "def setup_video_env(video_folder: str):\n",
    "    base_env = safety_gymnasium.make(\n",
    "        \"SafetyPointGoal1-v0\",\n",
    "        render_mode=\"rgb_array\",\n",
    "        camera_name=\"track\",  # Third-person chase camera view\n",
    "    )\n",
    "    env = SafetyGymnasium2Gymnasium(base_env)\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=video_folder,\n",
    "        episode_trigger=lambda episode_id: episode_id == 0,  # Record only the first episode.\n",
    "        name_prefix=\"cpo_rollout_3rd\",\n",
    "        disable_logger=True,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main script: load model, run rollout, and plot evaluation results.\n",
    "if __name__ == \"__main__\":\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Define the checkpoint path based on your directory structure.\n",
    "    checkpoint_path = (\n",
    "        \"./cpo_logs/train/CPO-{SafetyPointGoal1-v0}/seed-000-2025-04-13-22-53-59/\"\n",
    "        \"torch_save/epoch-50.pt\"\n",
    "    )\n",
    "    \n",
    "    # Print debug info: current working directory and checkpoint folder contents.\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    ckpt_folder = os.path.dirname(checkpoint_path)\n",
    "    if os.path.exists(ckpt_folder):\n",
    "        print(f\"Contents of '{ckpt_folder}':\", os.listdir(ckpt_folder))\n",
    "    else:\n",
    "        print(f\"Folder '{ckpt_folder}' does not exist.\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Create a temporary environment to determine observation and action dimensions.\n",
    "    temp_env = safety_gymnasium.make(\"SafetyPointGoal1-v0\", render_mode=None)\n",
    "    obs_space = temp_env.observation_space\n",
    "    if hasattr(obs_space, 'shape'):\n",
    "        obs_dim = obs_space.shape[0]\n",
    "    else:\n",
    "        # If the observation space is a dict, adjust accordingly (e.g., use obs_space['obs'])\n",
    "        raise ValueError(\"Unexpected observation space format. Adjust the code accordingly.\")\n",
    "    action_dim = temp_env.action_space.shape[0]\n",
    "    temp_env.close()\n",
    "    print(f\"Observation dimension: {obs_dim}, Action dimension: {action_dim}\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Load the trained CPO model.\n",
    "    model_wrapper = load_cpo_model(checkpoint_path, obs_dim, action_dim)\n",
    "    print(\"✅ Successfully loaded the CPO model.\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Set up the video environment.\n",
    "    video_folder = \"./cpo_logs/video\"\n",
    "    env = setup_video_env(video_folder)\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Run a rollout of approximately 30 seconds (900 steps at 30 FPS).\n",
    "    obs, info = env.reset(seed=0)\n",
    "    for step in range(900):\n",
    "        action, _ = model_wrapper.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if done or truncated:\n",
    "            print(f\"Episode terminated at step {step + 1}.\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    video_path = os.path.join(video_folder, \"cpo_rollout_3rd-episode-0.mp4\")\n",
    "    print(f\"✅ Video saved → {video_path}\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Load evaluation data and plot the performance graph.\n",
    "    eval_log_path = \"./cpo_logs/evaluations.npz\"\n",
    "    if not os.path.exists(eval_log_path):\n",
    "        print(f\"Evaluation log file not found at {eval_log_path}. Skipping evaluation plot.\")\n",
    "    else:\n",
    "        try:\n",
    "            data = np.load(eval_log_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading evaluation logs: {e}\")\n",
    "\n",
    "        timesteps    = data[\"timesteps\"]    # 1D array of timesteps.\n",
    "        results      = data[\"results\"]      # 2D array (evaluations x episodes)\n",
    "        mean_rewards = results.mean(axis=1)\n",
    "        std_rewards  = results.std(axis=1)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(timesteps, mean_rewards, label=\"Mean Reward\")\n",
    "        plt.fill_between(\n",
    "            timesteps,\n",
    "            mean_rewards - std_rewards,\n",
    "            mean_rewards + std_rewards,\n",
    "            alpha=0.3,\n",
    "            label=\"±1 σ\"\n",
    "        )\n",
    "        plt.xlabel(\"Environment Steps\")\n",
    "        plt.ylabel(\"Episode Reward\")\n",
    "        plt.title(\"CPO – SafetyPointGoal1 Evaluation\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test PPO et CPO par Omnisafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test installation safety-gymnasium\n",
    "import safety_gymnasium\n",
    "env = safety_gymnasium.make('SafetyPointGoal1-v0')\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    act = env.action_space.sample()\n",
    "    obs, reward, cost, terminated, truncated, info = env.step(act)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
